---
layout: page
title: Materials
permalink: /materials/
---

## Lecture Notes
The lecture notes are uploaded through the semester. For each chapter, the notes are provided section by section. 
### Chapter 0: Course Overview and Logistics
* [Handouts]({{site.baseurl}}/assets/Notes/CH0/CH0.pdf): All Sections included in a single file

### Chapter 1: Fundamentals of Deep Learning
* [Section 1]({{site.baseurl}}/assets/Notes/CH1/CH1_Sec1.pdf): Motivation to Learn DL
* [Section 2]({{site.baseurl}}/assets/Notes/CH1/CH1_Sec2.pdf): Learning from Data: _Basics_
* [Section 3]({{site.baseurl}}/assets/Notes/CH1/CH1_Sec3.pdf): Perceptron Machine
* [Section 4]({{site.baseurl}}/assets/Notes/CH1/CH1_Sec4.pdf): Deep Neural Networks
* [Section 5]({{site.baseurl}}/assets/Notes/CH1/CH1_Sec5.pdf): Function Optimization

### Chapter 2: Feedforward NNs
* [Section 1]({{site.baseurl}}/assets/Notes/CH2/CH2_Sec1.pdf): Forward Pass in MLPs
* [Section 2]({{site.baseurl}}/assets/Notes/CH2/CH2_Sec2.pdf): Computing Gradient via _Backpropagation_ on Computation Graph
* [Section 3]({{site.baseurl}}/assets/Notes/CH2/CH2_Sec3.pdf): Multiclass Classification
* [Section 4]({{site.baseurl}}/assets/Notes/CH2/CH2_Sec4.pdf): Mini-batch Training and SGD Algorithm

### Chapter 3: Advances - _Part I_
* [Section 1]({{site.baseurl}}/assets/Notes/CH3/CH3_Sec1.pdf): More on Optimizers
* [Section 2]({{site.baseurl}}/assets/Notes/CH3/CH3_Sec2.pdf): Overfitting, Regularization and Dropout
* [Section 3]({{site.baseurl}}/assets/Notes/CH3/CH3_Sec3.pdf): Data Distribution and Preporcessing
* [Section 4]({{site.baseurl}}/assets/Notes/CH3/CH3_Sec4.pdf): Standardization and Batch Normalization

### Chapter 4: Convolutional NNs
* [Section 1]({{site.baseurl}}/assets/Notes/CH4/CH4_Sec1.pdf): Why Convolution?
* [Section 2]({{site.baseurl}}/assets/Notes/CH4/CH4_Sec2.pdf): Components of CNNs
* [Section 3]({{site.baseurl}}/assets/Notes/CH4/CH4_Sec3.pdf): Deep CNNs
* [Section 4]({{site.baseurl}}/assets/Notes/CH4/CH4_Sec4.pdf): Training CNNs

### Chapter 5: Residual Learning
* [Section 1]({{site.baseurl}}/assets/Notes/CH5/CH5_Sec1.pdf): Depth and Vanishing Gradient
* [Section 2]({{site.baseurl}}/assets/Notes/CH5/CH5_Sec2.pdf): Skip Connection and ResNet

### Chapter 6: Sequence Processing
* [Section 1]({{site.baseurl}}/assets/Notes/CH6/CH6_Sec1.pdf): Sequence Data 
* [Section 2]({{site.baseurl}}/assets/Notes/CH6/CH6_Sec2.pdf): RNNs
* [Section 3]({{site.baseurl}}/assets/Notes/CH6/CH6_Sec3.pdf): Training RNNs 
* [Section 4]({{site.baseurl}}/assets/Notes/CH6/CH6_Sec4.pdf): Gating
* [Section 5]({{site.baseurl}}/assets/Notes/CH6/CH6_Sec5.pdf): Bidirectional Sequence Processing 
* [Section 6]({{site.baseurl}}/assets/Notes/CH6/CH6_Sec6.pdf): CTC Algorithm

### Chapter 7: Sequence to Sequence Models
* [Section 1]({{site.baseurl}}/assets/Notes/CH7/CH7_Sec1.pdf): Seq2Seq
* [Section 2]({{site.baseurl}}/assets/Notes/CH7/CH7_Sec2.pdf): Encoder Decoder
* [Section 3]({{site.baseurl}}/assets/Notes/CH7/CH7_Sec3.pdf): Attention 
* [Section 4]({{site.baseurl}}/assets/Notes/CH7/CH7_Sec4.pdf): Self-Attention and Transformer

### Chapter 8: Representation and Generation
* [Section 1]({{site.baseurl}}/assets/Notes/CH8/CH8_Sec1.pdf): Representation Problem
* [Section 2]({{site.baseurl}}/assets/Notes/CH8/CH8_Sec2.pdf): Autoencoding
* [Section 3]({{site.baseurl}}/assets/Notes/CH8/CH8_Sec3.pdf): Data Generation



## Tutorial Notebooks
The tutorial notebooks can be accessed below.
* [Tutorial 1](https://github.com/seyedsaleh/AplDL-tutorials-ece1508/blob/main/Tutorial_Notebooks/Tutorial1-Python-and-ML-Fundamentals.ipynb): Intro to Python, Basic ML in Python, by __Saleh Tabatabaei__
* [Tutorial 2](https://github.com/seyedsaleh/AplDL-tutorials-ece1508/blob/main/Tutorial_Notebooks/Tutorial2-PyTorch.ipynb): Intro to PyTorch, Auto-grad, by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/a5ad772e800fc000c61aaea3a2f3a510)
* [Tutorial 3](https://github.com/seyedsaleh/AplDL-tutorials-ece1508/blob/main/Tutorial_Notebooks/Tutorial3-Overfitting%2C%20Dropout%2C%20BatchNorm%2C%20L1%20and%20L2%20Regularization.ipynb): Underfitting and Overfitting: How to Prevent Them, by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/b9c9fc747655d341a11400fa42cd85b6)
* [Tutorial 4](https://github.com/seyedsaleh/AplDL-tutorials-ece1508/tree/main/Tutorial_Notebooks/Tutorial4-CNN%2C%20AlexNet%20in%20PyTorch): CNNs by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/5814f9ece8b5d1687112f32649621720)
* [Tutorial 5](https://q.utoronto.ca/courses/396459/modules): Midterm Review by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/d095763335f9139a5e7d99501e07cfd3)
* [Tutorial 6](https://q.utoronto.ca/courses/396459/modules): ResNet by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/515c9ee8c3a9d1f64a51377c68009a0d)
* [Tutorial 7](https://q.utoronto.ca/courses/396459/modules): RNNs by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/315c11f3172eb92eb08c222645bad5f3)
* [Tutorial 8](https://q.utoronto.ca/courses/396459/modules): Sequence Models by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/f0d28f4da270c61f7901555434ec0a42)
* [Tutorial 9](https://q.utoronto.ca/courses/396459/modules): Autoencoders by __Saleh Tabatabaei__ [Watch the Video](https://play.library.utoronto.ca/watch/87336a08b49ffa94c6b5ad6a7863b033)




## Book

There is indeed no single textbook for this course, and we use various resources in the course. The following textbooks have covered the key notions in the course. 

* [[GYC] Goodfellow, Ian, et al. _Deep Learning._ MIT Press, 2016.](https://www.deeplearningbook.org/)
* [[BB] Bishop, Christopher M., and Hugh Bishop. _Deep Learning: Foundations and Concepts._ Springer Nature, 2023.](https://www.bishopbook.com/)
* [[Ag] C. Aggarwal. _Neural Networks and Deep Learning._ Springer, 2018.](https://link.springer.com/book/10.1007/978-3-319-94463-0)

The following textbooks are also good resources for __practicing hands-on skills.__ Note that we are __not__ simply learning to implement only! We study the fundamentals of deep learning. Of course, we try to get our hands dirty as well and learn how to do implementation.

* [Chollet, Francois. _Deep learning with Python._ Manning Publications, 2021.](https://www.manning.com/books/deep-learning-with-python)
* [Müller, Andreas, and Sarah Guido. _Introduction to Machine Learning with Python._ O'Reilly Media, Inc., 2016.](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/)


## Reading List

This section will be completed gradually through the semester.


### Chapter 1: Preliminaries
#### Introduction to DL
* [Motivation](https://www.bishopbook.com/): Chapter 1 - Section 1.1 of [[BB]](https://www.bishopbook.com/)

#### ML Components
* [Review on Linear Algebra](https://www.deeplearningbook.org/): Chapter 2 of [[GYC]](https://www.deeplearningbook.org/)
* [ML Components](https://www.bishopbook.com/): Chapter 1 - Sections 1.2.1 to 1.2.4 of [[BB]](https://www.bishopbook.com/)
* [ML Basics](https://www.deeplearningbook.org/): Chapter 5 of [[GYC]](https://www.deeplearningbook.org/)

#### Review on Probability Theory
* [Probability Theory](https://www.bishopbook.com/): Chapter 2 of [[BB]](https://www.bishopbook.com/)
* [Probability Review](https://www.deeplearningbook.org/): Chapter 3 of [[GYC]](https://www.deeplearningbook.org/)


#### Classification Problem
* [Binary Classification](https://www.bishopbook.com/): Chapter 5 - Sections 5.1 and 5.2 of [[BB]](https://www.bishopbook.com/)
* [McCulloch-Pitts Model](https://link.springer.com/article/10.1007/BF02478259): Paper _A logical calculus of the ideas immanent in nervous activity_ published in the _Bulletin of Mathematical Biophysics_ by _Warren McCulloch and Walter Pitts_ in 1943, proposing a computational model for neuron. This paper is treated as the pioneer study leading to the idea of _artificial neuron_

#### Training via Risk Minimization
* [Overview on Risk Minimization](https://ieeexplore.ieee.org/abstract/document/788640): Paper _An overview of statistical learning theory_ published as an overview of his life-going developments in ML in the _IEEE Transactions on Neural Networks_ by _Vladimir N. Vapnik_ in 1999

#### Perceptron Algorithm 
* [Perceptron Simulation Experiments](https://ieeexplore.ieee.org/document/4066017): Paper _Perceptron Simulation Experiments_ presented by _Frank Rosenblatt_ in Proceedings of IRE in 1960
* [Perceptron](https://link.springer.com/book/10.1007/978-3-319-94463-0): Chapter 1 - Section 1.2.1 of [[Ag]](https://link.springer.com/book/10.1007/978-3-319-94463-0)

#### Universal Approximation Theorem
* [Universal Approximation](https://link.springer.com/article/10.1007/BF02551274): Paper _Approximation by superpositions of a sigmoidal function_ published in _Mathematics of Control, Signals and Systems_ by _George V. Cybenko_ in 1989

#### Deep NNs
* [DNNs](https://www.bishopbook.com/): Chapter 6 - Sections 6.2 and 6.3 of [[BB]](https://www.bishopbook.com/)

#### Optimization via Gradient Descent
* [Gradient-based Optimization](https://www.deeplearningbook.org/): Chapter 4 - Sections 4.3 and 4.4 of [[GYC]](https://www.deeplearningbook.org/)
* [Gradient Descent](https://www.bishopbook.com/): Chapter 7 - Sections 7.1 to 7.2 of [[BB]](https://www.bishopbook.com/)


### Chapter 2: Fully-connected FNNs
#### Forward Propagation
* [Deep FNNs](https://www.deeplearningbook.org/): Chapter 6 - Sections 6.3 and 6.4 of [[GYC]](https://www.deeplearningbook.org/)

#### Backpropagation
* [Backpropagation](https://www.deeplearningbook.org/): Chapter 6 - Section 6.5 of [[GYC]](https://www.deeplearningbook.org/)
* [Backpropagation](https://www.bishopbook.com/): Chapter 8 of [[BB]](https://www.bishopbook.com/)
* [Backpropagation of Error](https://www.nature.com/articles/323533a0) Paper _Learning representations by back-propagating errors_ published in _Nature_ by _D. Rumelhart, G. Hinton and R. Williams_ in 1986 advocating the idea of systematic gradient computation of a computation graph

#### Multi-class Classification
* [Binary Classification](https://www.bishopbook.com/): Chapter 6 Section 6.6 of [[BB]](https://www.bishopbook.com/)
* [Multi-class Models](https://link.springer.com/book/10.1007/978-3-319-94463-0): Chapter 2 - Section 2.3 of [[Ag]](https://link.springer.com/book/10.1007/978-3-319-94463-0)

#### Full-batch, sample-level and mini-batch SGD
* [SGD](https://www.deeplearningbook.org/): Chapter 5 - Section 5.9 of [[GYC]](https://www.deeplearningbook.org/)
* [SGD](https://www.bishopbook.com/): Chapter 7 - Section 7.2 of [[BB]](https://www.bishopbook.com/)

#### Generalization
* [Generalization](https://arxiv.org/abs/2102.05242): Chapter 6 of the Book _Patterns, predictions, and actions: A story about machine learning_ by _Moritz Hardt and B. Recht_ published in 2021

### Chapter 3: Optimizers, Regularization and Data
#### More on Optimizers
* [Learning Rate Scheduling](https://ieeexplore.ieee.org/abstract/document/7926641) Paper _Cyclical Learning Rates for Training Neural Networks_ published in _Winter Conference on Applications of Computer Vision (WACV)_ by _Leslie N. Smith_ in 2017 discussing learning rate scheduling
* [Rprop]() Paper _A direct adaptive method for faster backpropagation learning: the RPROP algorithm_ published in _IEEE International Conference on Neural Networks_ by _M. Riedmiller and H. Braun_ in 1993 proposing Rprop algorithm
* [RMSprop](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) Lecture note by _GEoffrey Hinton_ proposing RMSprop
* [RMSprop Analysis](https://arxiv.org/abs/1502.04390v1) Paper _RMSProp and equilibrated adaptive learning rates for non-convex optimization_ by _Y. Dauphin et al._ published in 2015 talking about RMSprop and citing  Honton's lecture notes
* [Adam](https://arxiv.org/abs/1412.6980) Paper _Adam: A Method for Stochastic Optimization_ published in 2014 by _D. Kingma and J. Ba_ proposing Adam
* [Notes on Optimizers](https://optmlclass.github.io/notes/optforml_notes.pdf) Lecture notes of the course _Optimization for Machine Learning_ by _Ashok Cutkosky_ in _Boston University_: A good resource for optimizers

#### Overfitting and Regularization
* [Regularization](https://www.deeplearningbook.org/): Chapter 7 of [[GYC]](https://www.deeplearningbook.org/)
* [Overfitting and Regularization](https://www.bishopbook.com/): Chapter 9 - Sections 9.1 to 9.3 of [[BB]](https://www.bishopbook.com/)
* [Tikhonov](https://epubs.siam.org/doi/abs/10.1137/S0895479897326432) Paper _Tikhonov Regularization and Total Least Squares_ published in 1999 by _G. Golub et al._ illustrating the Tikhonov Regularization work
* [Lasso](https://academic.oup.com/jrsssb/article/58/1/267/7027929) Paper _Regression Shrinkage and Selection Via the Lasso_ published in 1996 by _R. Tibshirani_ proposing the legendary Lasso 
* [Dropout 1](https://arxiv.org/abs/1207.0580) Paper _Improving neural networks by preventing co-adaptation of feature detectors_ published in 2012 by _G. Hinton et al._ proposing Dropout
* [Dropout 2](https://jmlr.org/papers/v15/srivastava14a.html) Paper _Dropout: A Simple Way to Prevent Neural Networks from Overfitting_ published in 2014 by _N. Srivastava et al._ providing some analysis and illustrations on Dropout

#### Data: Data Distribution, Data Cleaning, and Outliers
* [Data](https://arxiv.org/abs/2102.05242): Chapter 8 of the Book _Patterns, predictions, and actions: A story about machine learning_ by _Moritz Hardt and B. Recht_ published in 2021
* [Data Processing in Python](https://arxiv.org/abs/2211.04630) Open Book _Minimalist Data Wrangling with Python_ by _Marek Gagolewski_ going through data processing in Python

#### Normalization
* [Normalization](https://proceedings.neurips.cc/paper/2020/hash/9b8619251a19057cff70779273e95aa6-Abstract.html) Paper _Is normalization indispensable for training deep neural network?_ published in 2020 by _J. Shao et al._ discussing the meaning and effects of normalization
* [Batch-Norm](https://proceedings.mlr.press/v37/ioffe15.html) Paper _Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift_ published in 2015 by _S. Ioffe and C. Szegedy_ proposing Batch Normalization
* [Batch-Norm Meaning](https://proceedings.neurips.cc/paper/2018/hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html) Paper _How Does Batch Normalization Help Optimization?_ published in 2018 by _S. Santurkar et al._ discussing why Batch Normalization works: they claim that the main reason is that loss landscape is getting much smoother

### Chapter 4: Convolutional NNs
#### Development of CNNs
* [Hubel and Wiesel Study](https://pmc.ncbi.nlm.nih.gov/articles/PMC1359523/) Paper _Receptive fields, binocular interaction and functional architecture in the cat's visual cortex_ published in 1962 by _D. Hubel and T. Wiesel_ elaborating their finding on visual understanding
* [Neocognitron](https://link.springer.com/article/10.1007/BF00344251) Paper _Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position_ published in 1980 by _K. Fukushima _ proposing the Neocognitron as a computational model for visual learning
* [Backpropagating on LeNet](https://direct.mit.edu/neco/article/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code) Paper _Backpropagation Applied to Handwritten Zip Code Recognition_ published in 1989 by _Y. LeCun et al._ developing backpropagation for LeNet
* [LeNet](https://link.springer.com/article/10.1007/BF00344251) Paper _Gradient-Based Learning Applied to Document Recognition_ published in 1998 by _Y. LeCun et al._ discussing LeNet

#### Components of CNN
* [Convolution](https://www.deeplearningbook.org/): Chapter 9 - Sections 9.1 and 9.2 of [[GYC]](https://www.deeplearningbook.org/)
* [Convolution](https://www.bishopbook.com/): Chapter 10 - Sections 10.2.1 and 10.2.2 of [[BB]](https://www.bishopbook.com/)
* [Multi-channel Convolution](https://www.bishopbook.com/): Chapter 10 - Sections 10.2.3 to 10.2.5 of [[BB]](https://www.bishopbook.com/)
* [Pooling](https://www.deeplearningbook.org/): Chapter 9 - Sections 9.3 of [[GYC]](https://www.deeplearningbook.org/)
* [Pooling](https://www.bishopbook.com/): Chapter 10 - Section 10.2.6 of [[BB]](https://www.bishopbook.com/)
* [Flattening](https://www.bishopbook.com/): Chapter 10 - Sections 10.2.7 and 10.2.8 of [[BB]](https://www.bishopbook.com/)

#### Deep CNNs
* [Convolution](https://www.deeplearningbook.org/): Chapter 9 - Sections 9.4 and 9.6 of [[GYC]](https://www.deeplearningbook.org/)
* [VGG](https://arxiv.org/abs/1409.1556) Paper _Very Deep Convolutional Networks for Large-Scale Image Recognition_ published in 2014 by _K. Simonyan and A. Zisserman_ proposing VGG Architectures

#### Backpropagation on CNN
* [LeCun's Paper](https://ieeexplore.ieee.org/document/726791) Paper _Gradient-based learning applied to document recognition_ published in 2002 by _Y. LeCun et al._ summarizing the learning process in CNN
* [Efficient Backpropagation on CNN](https://inria.hal.science/inria-00112631/PDF/p1038112283956.pdf) Paper _High Performance Convolutional Neural Networks for Document Processing_ published in 2006 by _K. Chellapilla et al._  discussing efficient backpropagation on CNNs.

### Chapter 5: Residual Learning
* [ResNet](https://arxiv.org/abs/1512.03385) Paper _Deep Residual Learning for Image Recognition_ published in 2015 by _K. He et al._ proposing ResNet
* [ResNet-1001](https://arxiv.org/abs/1603.05027) Paper _Identity Mappings in Deep Residual Networks_ published in 2016 by _K. He et al._ demonstrating how deep ResNet can go
* [U-Net](https://arxiv.org/abs/1505.04597) Paper _U-Net: Convolutional Networks for Biomedical Image Segmentation_ published in 2015 by _O. Ronneberger et al._ proposing U-Net
* [DenseNet](https://arxiv.org/abs/1608.06993) Paper _Densely Connected Convolutional Networks_ published in 2017 by _H. Huang et al._ proposing DenseNet

### Chapter 6: Sequence Processing via NNs

#### Basics of Sequence Processing
* [Jordan Network](https://escholarship.org/uc/item/1fg2j76h) Paper _Attractor dynamics and parallelism in a connectionist sequential machine_ published in 1986 by _M. Jordan_ proposing his RNN
* [Elman Network](https://doi.org/10.1207/s15516709cog1402_1) Paper _Finding structure in time_ published in 1990 by _J. Elman_ proposing a revision to Jordan Network
* [Seq Models](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) Article _The Unreasonable Effectiveness of Recurrent Neural Networks_ written in May 2015 by _A. Karpathy_ discussing different types of sequence problems

#### Backpropagation Through Time
* [BPTT](https://ieeexplore.ieee.org/abstract/document/58337) Paper _Backpropagation through time: What it does and how to do it_ published in 2002 by _P. Werbos_ explaining BPTT
* [Vanishing Gradient with BPTT](https://proceedings.mlr.press/v28/pascanu13.html) Paper _On the difficulty of training recurrent neural networks_ published in 2013 by _R. Pascanu et al._ discussing challenges in training with BPTT
* [Truncated BPTT](https://direct.mit.edu/neco/article/2/4/490/5561/An-Efficient-Gradient-Based-Algorithm-for-On-Line) Paper _An efficient gradient-based algorithm for on-line training of recurrent network trajectories_ published in 1990 by _R. Williams and J. Peng_ explaining truncated BPTT

#### Gating
* [Gating Principle](https://link.springer.com/chapter/10.1007/978-3-642-24797-2_4) Chapter _Long Short-Term Memory_ published in 2012 in book _Supervised Sequence Labelling with Recurrent Neural Networks_ by _A. Graves_ explaining Gating idea
* [LSTM](https://ieeexplore.ieee.org/abstract/document/6795963) Paper _Long short-term memory_ published in 1997 by _S. Hochreiter and J. Schmidhuber_ proposing LSTM
* [GRU](https://arxiv.org/abs/1409.1259) Paper _On the Properties of Neural Machine Translation: Encoder-Decoder Approaches_ published in 2014 by _K. Cho et al._ proposing GRU

#### CTC Algorithm
* [CTC](https://dl.acm.org/doi/10.1145/1143844.1143891) Paper _Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks_ published in 2006 by _A. Graves et al._ proposing CTC Algorithm


### Chapter 7: Seq2Seq Models
#### Language Models
* [Basic LM](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) Paper _A Neural Probabilistic Language Model_ published in 2003 by _Y. Bengio_ developing frist really-functioning LM
* [RNN-LM](https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.pdf) Paper _Recurrent neural network based language model_ published in INTERSPEECH 2010 by _T. T Mikolov et al._ proposing a practical LM using RNNs

#### Seq2Seq Models
* [Seq2Seq](https://proceedings.neurips.cc/paper_files/paper/2014/hash/5a18e133cbf9f257297f410bb7eca942-Abstract.html) Paper _Sequence to Sequence Learning with Neural Networks_ published in 2014 by _I. Sutskever et al._ proposing end-to-end encoding and decoding

#### NMT and Attention Mechanism
* [Attention](https://arxiv.org/abs/1409.0473) Paper _Neural Machine Translation by Jointly Learning to Align and Translate_ published in 2015 by _D. Bahdanau et al._ proposing Attention mechanism
* [Attention NMT](https://arxiv.org/abs/1508.04025) Paper _Effective Approaches to Attention-based Neural Machine Translation_ published in 2015 by _M. Loung et al._ proposing Attention-based NMT

#### Self-Attention and Transformers
* [Transformer](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) Paper _Attention is All You Need_ published in 2017 by _A. Vaswani et al._ proposing Transformer (cited 205K times to this date)
* [Transformers](https://www.bishopbook.com/): Chapter 12 of [[BB]](https://www.bishopbook.com/)


### Chapter 8: Autoencoding and Data Generation

#### Nonlinear PCA
* [Nonlinear PCA](https://www.bishopbook.com/): Chapter 16 [[BB]](https://www.bishopbook.com/)
* [Representation](https://ieeexplore.ieee.org/abstract/document/6790375) Paper _Nonlinear component analysis as a kernel eigenvalue problem_ published in 1998 by _B. Schölkopf et al._ 
* [Nonlinear PCA via NNs](https://web.archive.org/web/20111003131045id_/http://pca.narod.ru/2MainGorbanKeglWunschZin.pdf) Paper _Nonlinear Principal Component Analysis: Neural Network Models and Applications_ published in 2008 by _M. Scholz et al._ 


#### AEs
* [Deterministic AEs](https://www.bishopbook.com/): Chapter 19 - Section 19.1 of [[BB]](https://www.bishopbook.com/)
* [Vanilla AEs](https://dbirman.github.io/learn/hierarchy/pdfs/Hinton2006.pdf) Paper _Reducing the Dimensionality of Data with Neural Networks_ published in 2006 by _G. Hinton and R. Salakhutdinov_ 
* [Denoising AEs](https://dl.acm.org/doi/pdf/10.1145/1390156.1390294) Paper _Extracting and Composing Robust Features with Denoising Autoencoders_ published in 2008 by _P. Vincent et al._ 

#### Variational AEs
* [VAEs](https://www.bishopbook.com/): Chapter 19 - Section 19.2 of [[BB]](https://www.bishopbook.com/)
* [Variational AEs](https://indico.math.cnrs.fr/event/11377/attachments/4589/6915/18012024_Kingma-and-Welling-2022%20Auto-Encoding%20Variational%20Bayes.pdf) Paper _Auto-Encoding Variational Bayes_ published in 2008 by _D. Kingma and P. Welling_ proposing VAE idea
