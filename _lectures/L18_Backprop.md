---
type: lecture
date: 2025-09-23T11:10:00
title: "Lecture 18: Backpropagation over MLP"
tldr: "Backpropagation"
stat: lec
# for lectures stat: lec
description: A neural network can describe a computation graph. We use this fact to develop an algorithmic approach for gradient computation. This is known as backpropagation. We see how we can backpropagate overn an MLP and how we can use it to train.
videoID: qoPG4y2uon0 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 2 - Section 2]({{ site.baseurl }}/assets/Notes/CH2/CH2_Sec2.pdf) 

**Further Reads:**
* [Backpropagation](https://www.bishopbook.com/): Chapter 8 of [[BB]](https://www.bishopbook.com/)
* [Backpropagation of Error](https://www.nature.com/articles/323533a0) Paper _Learning representations by back-propagating errors_ published in _Nature_ by _D. Rumelhart, G. Hinton and R. Williams_ in 1986 advocating the idea of systematic gradient computation of a computation graph