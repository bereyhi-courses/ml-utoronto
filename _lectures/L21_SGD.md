---
type: lecture
date: 2025-09-26T12:10:00
title: "Lecture 21: Stochastic Gradient Descent"
tldr: "SGD - Part I"
stat: lec
# for lectures stat: lec
description: We see that full-batch training is in general computationally hard. We come up with a simple remedy for that, which is called sample-level training. However, it can lead to a repetitive behavior. We hence add a stochastic shuffling to this algorithm. This leads to the so called Stochastic Gradient Descent.
videoID: WhtnAb3Bl9o 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 2 - Section 4]({{ site.baseurl }}/assets/Notes/CH2/CH2_Sec2.pdf) 

**Further Reads:**
* [SGD](https://www.deeplearningbook.org/): Chapter 5 - Section 5.9 of [[GYC]](https://www.deeplearningbook.org/)