---
type: lecture
date: 2025-11-25T11:10:00
title: "Lecture 54: Self-Attention as Processing Unit"
tldr: "Transformers I"
stat: lec
# for lectures stat: lec
description: We can look at the attention mechanism as a processing unit. Interestingly, we can ignore the time connection and perform the complete process in parallel with high efficiency. This is the building block of the transformers. We introduce this module in this lecture and understand it's efficiency. 
videoID: 7wf2fb-GHk4 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 7 - Section 4]({{ site.baseurl }}/assets/Notes/CH7/CH7_Sec4.pdf) 

**Further Reads:**
* [Transformers](https://www.bishopbook.com/): Chapter 12 of [[BB]](https://www.bishopbook.com/)

