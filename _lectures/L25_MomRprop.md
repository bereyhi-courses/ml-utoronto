---
type: lecture
date: 2025-10-03T11:10:00
title: "Lecture 25: Optimizer Boosting -- Scheduling, Momentum and Rprop Ideas"
tldr: "Optimizers - Part II"
stat: lec
# for lectures stat: lec
description: We discuss three key ideas for improving stochastic optimizers. Namely, learning rate scheduling, using moving averages, and automatic dimension-dependent learning rate scheduling via Rprop. These schemes are key components of most current robust optimizers. 
videoID: Mkw0iLmlSdw 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 3 - Section 1]({{ site.baseurl }}/assets/Notes/CH3/CH3_Sec1.pdf) 

**Further Reads:**
* [Learning Rate Scheduling](https://ieeexplore.ieee.org/abstract/document/7926641) Paper _Cyclical Learning Rates for Training Neural Networks_ published in _Winter Conference on Applications of Computer Vision (WACV)_ by _Leslie N. Smith_ in 2017 discussing learning rate scheduling
* [Rprop]() Paper _A direct adaptive method for faster backpropagation learning: the RPROP algorithm_ published in _IEEE International Conference on Neural Networks_ by _M. Riedmiller and H. Braun_ in 1993 proposing Rprop algorithm