---
type: lecture
date: 2025-11-25T12:10:00
title: "Lecture 55: Transformers with Multi-head Attention, Positional Encoding and Masked Decoding"
tldr: "Transformers II"
stat: lec
# for lectures stat: lec
description: We can use the self attention module to build a deep sequence processor. This is what we call Transformer. In this lecture, we see how Transformers look like and what are the key tricks required for them to work. 
videoID: FbZ5a_suMoE 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 7 - Section 4]({{ site.baseurl }}/assets/Notes/CH7/CH7_Sec4.pdf) 

**Further Reads:**
* [Transformer](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) Paper _Attention is All You Need_ published in 2017 by _A. Vaswani et al._ proposing Transformer (cited 205K times to this date)