---
type: lecture
date: 2025-09-16T11:00:00
title: "Lecture 13: More on Gradient Descent"
tldr: "Function Optimization - Part II"
stat: lec
# for lectures stat: lec
description: We take a look at the behavior of gradient descent algorithm. We see that it always converge to a minimizer if we choose efficient learning rate. Also we learn how to handle cases with discontinuous or non-differentiable functions. 
videoID: -tCziMpYC20 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 1 - Section 5]({{ site.baseurl }}/assets/Notes/CH1/CH1_Sec5.pdf) 

**Further Reads:**
* [Gradient Descent](https://www.bishopbook.com/): Chapter 7 - Section 7.2 of [[BB]](https://www.bishopbook.com/)