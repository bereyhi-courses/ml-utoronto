---
type: lecture
date: 2025-11-14T12:00:00
title: "Lecture 47: Backpropagation Through Time"
tldr: "RNN II"
stat: lec
# for lectures stat: lec
description: We see that by sequential processing of data, we need to backpropagate through time. This leads to finite memory due to the vanishing gradient behavior. 
videoID: q3rwTYBEQ1s 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 6 - Section 3]({{ site.baseurl }}/assets/Notes/CH6/CH6_Sec3.pdf) 

**Further Reads:**
* [Vanishing Gradient with BPTT](https://proceedings.mlr.press/v28/pascanu13.html) Paper _On the difficulty of training recurrent neural networks_ published in 2013 by _R. Pascanu et al._ discussing challenges in training with BPTT
* [Truncated BPTT](https://direct.mit.edu/neco/article/2/4/490/5561/An-Efficient-Gradient-Based-Algorithm-for-On-Line) Paper _An efficient gradient-based algorithm for on-line training of recurrent network trajectories_ published in 1990 by _R. Williams and J. Peng_ explaining truncated BPTT



