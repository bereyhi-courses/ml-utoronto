---
type: lecture
date: 2025-09-19T12:10:00
title: "Lecture 17: Backward Pass on Computation Graph"
tldr: "Backward Pass - Part III"
stat: lec
# for lectures stat: lec
description: We start by the computation graph of the perceptron. We see that computing the sample gradient is equivalent to propagation of computations backward on the graph. This gives us the idea, how we can compute sample gradient on much larger networks. 
videoID: Zns_5lZTHQA 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 2 - Section 2]({{ site.baseurl }}/assets/Notes/CH2/CH2_Sec2.pdf) 

**Further Reads:**
* [Backpropagation](https://www.deeplearningbook.org/): Chapter 6 - Section 6.5 of [[GYC]](https://www.deeplearningbook.org/)
* [Backpropagation](https://www.bishopbook.com/): Chapter 8 of [[BB]](https://www.bishopbook.com/)