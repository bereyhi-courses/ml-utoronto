---
type: lecture
date: 2025-09-12T12:00:00
title: "Lecture 12: Iterative Optimization by Gradient Descent"
tldr: "Function Optimization - Part I"
stat: lec
# for lectures stat: lec
description: Gradient descent is the key algorithm enabling training of DNNs. We take a look at its foundation to understand how and why it works. 
videoID: 3JRGcLcbcWY 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 1 - Section 5]({{ site.baseurl }}/assets/Notes/CH1/CH1_Sec5.pdf) 

**Further Reads:**
* [Gradient-based Optimization](https://www.deeplearningbook.org/): Chapter 4 - Sections 4.3 and 4.4 of [[GYC]](https://www.deeplearningbook.org/)
* [Gradient Descent](https://www.bishopbook.com/): Chapter 7 - Sections 7.1 and 7.2 of [[BB]](https://www.bishopbook.com/)