---
type: lecture
date: 2025-10-14T11:10:00
title: "Lecture 34: Why Convolution?"
tldr: "CNN I"
stat: lec
# for lectures stat: lec
description: We start with CNNs. We see that convolution is a scanning process with a linear filter. This operation gives us a fort of shift invariance enabling us to accomplish visual learning tasks at significantly lower complexity.
videoID: aAJkNSzNm2g 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 4 - Section 1]({{ site.baseurl }}/assets/Notes/CH4/CH4_Sec1.pdf) 

**Further Reads:**
* [Hubel and Wiesel Study](https://pmc.ncbi.nlm.nih.gov/articles/PMC1359523/) Paper _Receptive fields, binocular interaction and functional architecture in the cat's visual cortex_ published in 1962 by _D. Hubel and T. Wiesel_ elaborating their finding on visual understanding
* [Neocognitron](https://link.springer.com/article/10.1007/BF00344251) Paper _Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position_ published in 1980 by _K. Fukushima _ proposing the Neocognitron as a computational model for visual learning
* [Backpropagating on LeNet](https://direct.mit.edu/neco/article/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code) Paper _Backpropagation Applied to Handwritten Zip Code Recognition_ published in 1989 by _Y. LeCun et al._ developing backpropagation for LeNet
* [LeNet](https://link.springer.com/article/10.1007/BF00344251) Paper _Gradient-Based Learning Applied to Document Recognition_ published in 1998 by _Y. LeCun et al._ discussing LeNet