---
type: lecture
date: 2025-11-21T11:10:00
title: "Lecture 52: Encoding-Decoding Architectures"
tldr: "Seq2Seq - Enc-Dec"
stat: lec
# for lectures stat: lec
description: We use our basic LM to build an Caption generator machine. This is a simple example of an encoding-decoding architecture. We study these architectures and see their functionality.
videoID: FatNgdJxONY 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 7 - Section 2]({{ site.baseurl }}/assets/Notes/CH7/CH7_Sec2.pdf) 

**Further Reads:**
* [Seq2Seq](https://proceedings.neurips.cc/paper_files/paper/2014/hash/5a18e133cbf9f257297f410bb7eca942-Abstract.html) Paper _Sequence to Sequence Learning with Neural Networks_ published in 2014 by _I. Sutskever et al._ proposing end-to-end encoding and decoding