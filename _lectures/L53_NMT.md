---
type: lecture
date: 2025-11-21T12:10:00
title: "Lecture 53: NMT and Attention Mechanism"
tldr: "Seq2Seq - Attention"
stat: lec
# for lectures stat: lec
description: We extend the Enc-Dec idea to NMT problem. We see that despite its theoretical soundness, it suffers from lack of attention. We hence build a computational mechanism that provides the decoder possibility to attend to related tokens at the encoder. This is what we know as Attention Mechanism. 
videoID: ZxlRximFCWc 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 7 - Section 3]({{ site.baseurl }}/assets/Notes/CH7/CH7_Sec3.pdf) 

**Further Reads:**
* [Attention](https://arxiv.org/abs/1409.0473) Paper _Neural Machine Translation by Jointly Learning to Align and Translate_ published in 2015 by _D. Bahdanau et al._ proposing Attention mechanism
* [Attention NMT](hhttps://arxiv.org/abs/1508.04025) Paper _Effective Approaches to Attention-based Neural Machine Translation_ published in 2015 by _M. Loung et al._ proposing Attention-based NMT

