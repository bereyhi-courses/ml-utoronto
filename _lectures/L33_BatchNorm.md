---
type: lecture
date: 2025-10-14T11:10:00
title: "Lecture 33: Batch Normalization"
tldr: "Batch-Norm"
stat: lec
# for lectures stat: lec
description: We study Batch Normalization. We see how the backpropagation gets impacted when we do batch normalization. 
videoID: 8TzEtF-HtrM  
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 3 - Section 4]({{ site.baseurl }}/assets/Notes/CH3/CH3_Sec4.pdf) 

**Further Reads:**
* [Batch-Norm](https://proceedings.mlr.press/v37/ioffe15.html) Paper _Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift_ published in 2015 by _S. Ioffe and C. Szegedy_ proposing Batch Normalization
* [Batch-Norm Meaning](https://proceedings.neurips.cc/paper/2018/hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html) Paper _How Does Batch Normalization Help Optimization?_ published in 2018 by _S. Santurkar et al._ discussing why Batch Normalization works: they claim that the main reason is that loss landscape is getting much smoother
