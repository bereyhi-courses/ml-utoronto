---
type: lecture
date: 2025-11-28T11:10:00
title: "Lecture 56: Representation via Nonlinear PCA"
tldr: "AEs I"
stat: lec
# for lectures stat: lec
description: We discuss the fundamental problem of representation learning. We see that using symmetric encoder and decoder we can learn an efficient representation known as latent. We see that this is indeed a nonlinear version of the PCA. 
videoID: izSBWIVYxS8 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 8 - Section 1]({{ site.baseurl }}/assets/Notes/CH8/CH8_Sec1.pdf) 

**Further Reads:**
* [Nonlinear PCA](https://www.bishopbook.com/): Chapter 16 [[BB]](https://www.bishopbook.com/)
* [Representation](https://ieeexplore.ieee.org/abstract/document/6790375) Paper _Nonlinear component analysis as a kernel eigenvalue problem_ published in 1998 by _B. Sch√∂lkopf et al._ 
* [Nonlinear PCA via NNs](https://web.archive.org/web/20111003131045id_/http://pca.narod.ru/2MainGorbanKeglWunschZin.pdf) Paper _Nonlinear Principal Component Analysis: Neural Network Models and Applications_ published in 2008 by _M. Scholz et al._ 