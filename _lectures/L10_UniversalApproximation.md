---
type: lecture
date: 2025-09-09T12:00:00
title: "Lecture 10: From Perceptron to NNs -- Universal Approximation"
tldr: "Intro to NNs - Part I"
stat: lec
# for lectures stat: lec
description: We are now ready to study NNs. We look into the example of learning XOR and see that a single Perceptron is not able to do it. We then build a network of perceptrons to learn XOR. This gives birth to the idea of NNs. We see that NNs are universal approximators, i.e., they can approximate any complicated function as accurate as we want if they are large enough. 
videoID: 5nPjctlO3pY 
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 1 - Section 3]({{ site.baseurl }}/assets/Notes/CH1/CH1_Sec3.pdf) 

**Further Reads:**
* [Universal Approximation](https://link.springer.com/article/10.1007/BF02551274): Paper _Approximation by superpositions of a sigmoidal function_ published in _Mathematics of Control, Signals and Systems_ by _George V. Cybenko_ in 1989